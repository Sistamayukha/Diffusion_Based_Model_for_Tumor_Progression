{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EGFqFNv4qit"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlzIzm9BFAgU",
        "outputId": "60f5935a-6d6c-4778-c05b-64f14d7acba3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Load zip from Google Drive\n",
        "zip_path = \"/content/drive/MyDrive/678_Team4_Dataset/ISPY1_Sample.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        print(\"✅ Extracted patch dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTbXJekJFlk7",
        "outputId": "1e428ae8-769d-44f2-ffdd-9ffdb1e5074e"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQnBFMaIKwHz"
      },
      "source": [
        "# DICOM Image Metadata Extraction and Organization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P38wGCsUIgUQ",
        "outputId": "d432d90a-1539-4958-e4b0-8e10f6d855da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define input file and output folder paths\n",
        "input_file = \"/content/drive/MyDrive/metadata.csv\"\n",
        "output_folder = \"/content/drive/MyDrive/image_metadata\"\n",
        "output_file = os.path.join(output_folder, \"image_metadata.csv\")\n",
        "\n",
        "# Step 2: Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Step 3: Load the original metadata\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Step 4: Fill missing or invalid 'Number of Images' values with 1\n",
        "df['Number of Images'] = pd.to_numeric(df['Number of Images'], errors='coerce').fillna(1).astype(int)\n",
        "\n",
        "# Step 5: Replicate rows based on 'Number of Images'\n",
        "df_expanded = df.loc[df.index.repeat(df['Number of Images'])].reset_index(drop=True)\n",
        "\n",
        "# Step 6: Select relevant columns for the refined output\n",
        "refined_df = df_expanded[[\n",
        "    'Subject ID',\n",
        "    'Study Date',\n",
        "    'Series Description',\n",
        "    'File Location',\n",
        "    'Study Description',\n",
        "    'Number of Images'\n",
        "]]\n",
        "\n",
        "# Step 7: Save the refined DataFrame as image_metadata.csv in image_metadata/ folder\n",
        "refined_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Refined metadata saved at: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMrsK8o8ODSM"
      },
      "source": [
        "#Creating labels to Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb5kqZitNsOr",
        "outputId": "26f148d1-5807-4d94-bbc9-f576c5f88cae"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to your zip file stored in Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/Tumor Progression ML Datasets/NPY_data1.zip'\n",
        "extract_folder = '/content/drive/MyDrive/Tumor Progression ML Datasets/NPY_data1/NPY_data1'\n",
        "\n",
        "# Check if the folder already exists (i.e., files are already extracted)\n",
        "if not os.path.exists(extract_folder):\n",
        "    # Unzip the file if the folder does not exist\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    print(f\"Extracted files to: {extract_folder}\")\n",
        "else:\n",
        "    print(f\"Files already extracted to: {extract_folder}\")\n",
        "\n",
        "# Now, define the directory containing the .npy files\n",
        "npy_dir = extract_folder  # Path to the folder where you extracted the files\n",
        "\n",
        "# Check if the .npy files are present by recursively listing all files in the directory\n",
        "def list_npy_files(directory):\n",
        "    npy_files = []\n",
        "    for root, dirs, files in os.walk(directory):  # os.walk recursively lists all files\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.npy'):  # Check if it's a .npy file\n",
        "                npy_files.append(os.path.join(root, file))  # Add the full file path\n",
        "    return npy_files\n",
        "\n",
        "# List all .npy files in the directory\n",
        "npy_files = list_npy_files(npy_dir)\n",
        "\n",
        "# Check if npy_files is empty\n",
        "if not npy_files:\n",
        "    print(\"No .npy files found in the extracted directory.\")\n",
        "else:\n",
        "    # Initialize empty lists for image data and labels\n",
        "    image_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define the target shape for resizing (e.g., 256x256)\n",
        "    target_shape = (256, 256)  # Define the shape to which you want to resize images\n",
        "\n",
        "    # Loop through each .npy file and assign label as 1 (tumor)\n",
        "    for file in npy_files:\n",
        "        # Load the image\n",
        "        img = np.load(file)\n",
        "\n",
        "        # Resize the image to ensure consistency in shape\n",
        "        if img.shape != target_shape:\n",
        "            img = np.resize(img, target_shape)  # Resize image to target shape\n",
        "\n",
        "        # Append the image data\n",
        "        image_data.append(img)\n",
        "\n",
        "        # Label all images as 1 (tumor) initially\n",
        "        labels.append(1)  # Tumor label\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    image_data = np.array(image_data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Display the first few labels for verification\n",
        "    print(f\"First 5 images and labels:\")\n",
        "    for i in range(5):\n",
        "        print(f\"Image {i+1} label: {labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dohbUHBZ6S2"
      },
      "source": [
        "#Saving images and Labels to folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVUCurSgZ5Nb",
        "outputId": "aecd5191-3e86-4a1c-c25e-f3fc9a9879c4"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Tumor Progression ML Datasets/image_data.npy', image_data)\n",
        "print(f\"Image data saved to /content/drive/MyDrive/Tumor Progression ML Datasets/image_data.npy\")\n",
        "\n",
        "np.save('/content/drive/MyDrive/Tumor Progression ML Datasets/labels.npy', labels)\n",
        "print(f\"Labels saved to /content/drive/MyDrive/Tumor Progression ML Datasets/labels.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0JzUFdTaF65"
      },
      "source": [
        "#Splitting the Data into Training, Testing and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9e1VckXaCYN",
        "outputId": "46e33406-106c-4bb0-da49-1b911494db11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths to the saved .npy files\n",
        "image_data_path = '/content/drive/MyDrive/Tumor Progression ML Datasets/image_data.npy'\n",
        "labels_path = '/content/drive/MyDrive/Tumor Progression ML Datasets/labels.npy'\n",
        "\n",
        "# Check if the files exist before loading\n",
        "if os.path.exists(image_data_path) and os.path.exists(labels_path):\n",
        "    # Load the .npy files\n",
        "    image_data = np.load(image_data_path)\n",
        "    labels = np.load(labels_path)\n",
        "\n",
        "    print(\"Image data and labels successfully loaded.\")\n",
        "else:\n",
        "    print(\"Image data or labels file does not exist.\")\n",
        "\n",
        "# Now perform the data splitting\n",
        "# Split the data into 80% training, 10% validation, and 10% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(image_data, labels, test_size=0.2, random_state=42)  # 80% for training\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 50% of the remaining, 20% for validation and test\n",
        "\n",
        "# Verify the shapes of the resulting splits\n",
        "print(f\"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}, Validation labels shape: {y_val.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ5_mtx8aMva"
      },
      "source": [
        "#Converting to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYUG4s9PaMDE",
        "outputId": "85c8ab46-df93-4201-d6ad-dca11c3d2861"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Labels as long type for classification\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader for batch processing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Verify DataLoader\n",
        "print(f\"Training loader size: {len(train_loader)} batches\")\n",
        "print(f\"Validation loader size: {len(val_loader)} batches\")\n",
        "print(f\"Test loader size: {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-9A2YeaSuA"
      },
      "source": [
        "#Printing shapes and lables of sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iANYnZlXaRTw",
        "outputId": "7cc5da30-78e5-40c0-97a2-e04534c62830"
      },
      "outputs": [],
      "source": [
        "# Inspect some samples from each dataset\n",
        "print(f\"First 5 training samples:\")\n",
        "for i in range(5):\n",
        "    print(f\"Image {i+1} shape: {X_train[i].shape}, Label: {y_train[i]}\")\n",
        "\n",
        "print(f\"First 5 validation samples:\")\n",
        "for i in range(5):\n",
        "    print(f\"Image {i+1} shape: {X_val[i].shape}, Label: {y_val[i]}\")\n",
        "\n",
        "print(f\"First 5 test samples:\")\n",
        "for i in range(5):\n",
        "    print(f\"Image {i+1} shape: {X_test[i].shape}, Label: {y_test[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAWE9GavaaKM"
      },
      "source": [
        "# Saving tensors as .pt files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_0nveg2aZkI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(X_train_tensor, 'X_train.pt')\n",
        "torch.save(y_train_tensor, 'y_train.pt')\n",
        "torch.save(X_val_tensor, 'X_val.pt')\n",
        "torch.save(y_val_tensor, 'y_val.pt')\n",
        "torch.save(X_test_tensor, 'X_test.pt')\n",
        "torch.save(y_test_tensor, 'y_test.pt')\n",
        "\n",
        "# To load the data back later\n",
        "X_train_tensor = torch.load('X_train.pt')\n",
        "y_train_tensor = torch.load('y_train.pt')\n",
        "torch.save(X_train_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/X_train.pt')\n",
        "torch.save(y_train_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/y_train.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQJJ0PtnaeKE"
      },
      "outputs": [],
      "source": [
        "torch.save(X_val_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/X_val.pt')\n",
        "torch.save(y_val_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/y_val.pt')\n",
        "torch.save(X_test_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/X_test.pt')\n",
        "torch.save(y_test_tensor, '/content/drive/MyDrive/Tumor Progression ML Datasets/y_test.pt')\n",
        "\n",
        "# To load the data back later with safer loading\n",
        "X_train_tensor = torch.load('/content/drive/MyDrive/Tumor Progression ML Datasets/X_train.pt', weights_only=True)\n",
        "y_train_tensor = torch.load('/content/drive/MyDrive/Tumor Progression ML Datasets/y_train.pt', weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMvxF1Ri5eOt"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3BCzpwXG06D",
        "outputId": "d3c0e5c5-0fde-45a1-f753-cf2755d5bd24"
      },
      "outputs": [],
      "source": [
        "# # ⬇️ Load zip from Google Drive\n",
        "# zip_path = \"/content/drive/MyDrive/678_Team4_Dataset/X_train.pt.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# if zipfile.is_zipfile(zip_path):\n",
        "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extract_path)\n",
        "#         print(\"✅ Extracted patch dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKAZe4zLM2tt",
        "outputId": "5a61390c-261e-4d63-c1d9-eb4b5eae14dd"
      },
      "outputs": [],
      "source": [
        "# # ⬇️ Load zip from Google Drive\n",
        "# zip_path = \"/content/drive/MyDrive/678_Team4_Dataset/X_test.pt.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# if zipfile.is_zipfile(zip_path):\n",
        "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extract_path)\n",
        "#         print(\"✅ Extracted patch dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwaTXyQcM4fh",
        "outputId": "74b2775d-934b-43c7-aa0e-495df14ea1cb"
      },
      "outputs": [],
      "source": [
        "# # ⬇️ Load zip from Google Drive\n",
        "# zip_path = \"/content/drive/MyDrive/678_Team4_Dataset/X_val.pt.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# if zipfile.is_zipfile(zip_path):\n",
        "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extract_path)\n",
        "#         print(\"✅ Extracted patch dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f4AU0N43U_J",
        "outputId": "4514d11f-b2c5-447c-c32c-92da637444fd"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# src_path = '/content/drive/MyDrive/678_Team4_Dataset/y_train.pt'\n",
        "# dst_path = '/content'\n",
        "\n",
        "# shutil.copy(src_path, dst_path)\n",
        "# print(\"✅ File copied successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFQ36tTBM-X1",
        "outputId": "a22729c2-91a6-492e-d207-feb159a3c792"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# src_path = '/content/drive/MyDrive/678_Team4_Dataset/y_test.pt'\n",
        "# dst_path = '/content'\n",
        "\n",
        "# shutil.copy(src_path, dst_path)\n",
        "# print(\"✅ File copied successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkudJ_JgM_-c",
        "outputId": "42c85e60-a03d-4cfc-d357-955321d295b9"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# src_path = '/content/drive/MyDrive/678_Team4_Dataset/y_val.pt'\n",
        "# dst_path = '/content'\n",
        "\n",
        "# shutil.copy(src_path, dst_path)\n",
        "# print(\"✅ File copied successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxsmC9fL3-QQ",
        "outputId": "f89a656e-7f08-4ccb-a3c6-0934b1aa95e8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision pytorch-lightning tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li2fQgBH495V",
        "outputId": "80c29dfa-afc8-46a6-cf23-c095c8fb8f92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbp0XEL3a6Ag"
      },
      "source": [
        "### Loading and Preprocessing Image Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcIPDc0S5Jth",
        "outputId": "db516644-3323-427a-912a-a354c79085d5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Paths to the .pt files for train, validation, and test sets\n",
        "X_train_path = '/content/X_train.pt'\n",
        "y_train_path = '/content/y_train.pt'\n",
        "\n",
        "# Load the tensors from Google Drive\n",
        "X_train_tensor = torch.load(X_train_path)\n",
        "y_train_tensor = torch.load(y_train_path)\n",
        "\n",
        "\n",
        "def add_channel_dimension(tensor):\n",
        "    if len(tensor.shape) == 3:\n",
        "        tensor = tensor.unsqueeze(1)  # Add channel dimension (1 channel, grayscale)\n",
        "    if tensor.shape[1] == 1:  # If it has only 1 channel\n",
        "        tensor = tensor.repeat(1, 3, 1, 1)  # Repeat the grayscale image across 3 channels\n",
        "    return tensor\n",
        "\n",
        "# Add channel dimensions if necessary\n",
        "X_train_tensor = add_channel_dimension(X_train_tensor)\n",
        "\n",
        "# Verify the shape of the image tensors\n",
        "print(f\"Training images shape: {X_train_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOdtMau5a2f",
        "outputId": "dcf5b055-e368-42c7-aad3-9ddc61596a77"
      },
      "outputs": [],
      "source": [
        "X_val_path = '/content/X_val.pt'\n",
        "y_val_path = '/content/y_val.pt'\n",
        "\n",
        "X_val_tensor = torch.load(X_val_path)\n",
        "y_val_tensor = torch.load(y_val_path)\n",
        "\n",
        "def add_channel_dimension(tensor):\n",
        "    if len(tensor.shape) == 3:\n",
        "        tensor = tensor.unsqueeze(1)  # Add channel dimension (1 channel, grayscale)\n",
        "\n",
        "    if tensor.shape[1] == 1:  # If it has only 1 channel\n",
        "        tensor = tensor.repeat(1, 3, 1, 1)  # Repeat the grayscale image across 3 channels\n",
        "    return tensor\n",
        "\n",
        "X_val_tensor = add_channel_dimension(X_val_tensor)\n",
        "print(f\"Validation images shape: {X_val_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq3Vxj_85iha",
        "outputId": "4aa72e20-b3c3-480f-de1a-7b56eef3004a"
      },
      "outputs": [],
      "source": [
        "X_test_path = '/content/X_test.pt'\n",
        "y_test_path = '/content/y_test.pt'\n",
        "\n",
        "X_test_tensor = torch.load(X_test_path)\n",
        "y_test_tensor = torch.load(y_test_path)\n",
        "\n",
        "def add_channel_dimension(tensor):\n",
        "    if len(tensor.shape) == 3:\n",
        "        tensor = tensor.unsqueeze(1)  # Add channel dimension (1 channel, grayscale)\n",
        "    if tensor.shape[1] == 1:  # If it has only 1 channel\n",
        "        tensor = tensor.repeat(1, 3, 1, 1)  # Repeat the grayscale image across 3 channels\n",
        "    return tensor\n",
        "\n",
        "X_test_tensor = add_channel_dimension(X_test_tensor)\n",
        "print(f\"Test images shape: {X_test_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnYzhqM9bWXG"
      },
      "source": [
        "#Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYismyhho02V"
      },
      "outputs": [],
      "source": [
        "X_train = torch.load(\"/content/X_train.pt\", map_location=\"cpu\")\n",
        "y_train = torch.load(\"/content/y_train.pt\", map_location=\"cpu\")\n",
        "X_val = torch.load(\"/content/X_val.pt\", map_location=\"cpu\")\n",
        "y_val = torch.load(\"/content/y_val.pt\", map_location=\"cpu\")\n",
        "X_test = torch.load(\"/content/X_test.pt\", map_location=\"cpu\")\n",
        "y_test = torch.load(\"/content/y_test.pt\", map_location=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AZ0H4_obrfL"
      },
      "source": [
        "### Customing Tumor Dataset and DataLoader for Efficient Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kffVF7qt5pdY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TumorDataset(Dataset):\n",
        "    def __init__(self, image_tensor, label_tensor):\n",
        "        self.images = image_tensor\n",
        "        self.labels = label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        if img.ndim == 2:\n",
        "            img = img.unsqueeze(0)  # (H, W) → (1, H, W)\n",
        "        if img.size(0) == 1:\n",
        "            img = img.repeat(3, 1, 1)  # (1, H, W) → (3, H, W)\n",
        "        return img, self.labels[idx]\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_dataset = TumorDataset(image_tensor=X_train, label_tensor=y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "\n",
        "# Create a DataLoader for the validation dataset\n",
        "val_dataset = TumorDataset(image_tensor=X_val, label_tensor=y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "# To verify, print the first batch from the DataLoader\n",
        "for images, labels in train_loader:\n",
        "    print(f\"First batch - Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgoAT7yyb1hs"
      },
      "source": [
        "#UNet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcmBbLlO5tuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, init_features=64):\n",
        "        super(UNet, self).__init__()\n",
        "        features = init_features\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.dec4 = UNet._block(features * 16, features * 8, name=\"dec4\")\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.dec3 = UNet._block(features * 8, features * 4, name=\"dec3\")\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.dec2 = UNet._block(features * 4, features * 2, name=\"dec2\")\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.dec1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "\n",
        "        # Decoder with upsampling and skip connections\n",
        "        d4 = self.upconv4(b)\n",
        "        d4 = F.interpolate(d4, size=e4.shape[2:], mode='bilinear', align_corners=False)\n",
        "        d4 = torch.cat((d4, e4), dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.upconv3(d4)\n",
        "        d3 = F.interpolate(d3, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
        "        d3 = torch.cat((d3, e3), dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.upconv2(d3)\n",
        "        d2 = F.interpolate(d2, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
        "        d2 = torch.cat((d2, e2), dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = F.interpolate(d1, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
        "        d1 = torch.cat((d1, e1), dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, out_channels, name):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkhXYpNucDHI"
      },
      "source": [
        "#Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGWJTfutpwOS"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1TssqFcVp5"
      },
      "source": [
        "# DDPM Model for Tumor Progression Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-q45bV3pw0h"
      },
      "outputs": [],
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "    def __init__(self, num_steps=1000, lr=1e-4, T_max=100):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.num_steps = num_steps\n",
        "        self.betas = torch.linspace(1e-4, 0.02, num_steps)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alpha_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "\n",
        "        # Register buffers for efficient access\n",
        "        self.register_buffer('sqrt_alpha_cumprod', torch.sqrt(self.alpha_cumprod))\n",
        "        self.register_buffer('sqrt_one_minus_alpha_cumprod', torch.sqrt(1.0 - self.alpha_cumprod))\n",
        "\n",
        "        self.unet = UNet(in_channels=3, out_channels=3, init_features=64)\n",
        "        self.lr = lr\n",
        "        self.T_max = T_max\n",
        "\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.log_file = os.path.join(self.log_dir, 'loss_log.txt')\n",
        "        self.train_losses, self.val_losses = [], []\n",
        "\n",
        "    def forward(self, x, t=None, y=None):\n",
        "        return self.unet(x)\n",
        "\n",
        "    def forward_process(self, x_0, y):\n",
        "        \"\"\"\n",
        "        Apply the forward diffusion process to the input image.\n",
        "        Returns:\n",
        "        - noisy_images: The noisy image at a random timestep\n",
        "        - noise: The noise that was added\n",
        "        - t: The random timestep\n",
        "        \"\"\"\n",
        "        batch_size = x_0.size(0)\n",
        "        device = x_0.device\n",
        "\n",
        "        # Sample random timesteps for each image in the batch\n",
        "        t = torch.randint(0, self.num_steps, (batch_size,), device=device).long()\n",
        "\n",
        "        # Sample noise\n",
        "        noise = torch.randn_like(x_0)\n",
        "\n",
        "        # Get the alpha values for the sampled timesteps\n",
        "        sqrt_alpha = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "\n",
        "        # Add noise to the images\n",
        "        noisy_images = sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
        "\n",
        "        return noisy_images, noise, t\n",
        "\n",
        "    def reverse_process(self, x_t, t, y):\n",
        "        \"\"\"Apply the reverse process using the UNet\"\"\"\n",
        "        return self.unet(x_t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x_0, y = batch\n",
        "        noisy_x, noise, t = self.forward_process(x_0, y)\n",
        "        pred_noise = self.reverse_process(noisy_x, t, y)\n",
        "        loss = F.mse_loss(pred_noise, noise)\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x_0, y = batch\n",
        "        noisy_x, noise, t = self.forward_process(x_0, y)\n",
        "        pred_noise = self.reverse_process(noisy_x, t, y)\n",
        "        loss = F.mse_loss(pred_noise, noise)\n",
        "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x_0, y = batch\n",
        "        noisy_x, noise, t = self.forward_process(x_0, y)\n",
        "        pred_noise = self.reverse_process(noisy_x, t, y)\n",
        "        loss = F.mse_loss(pred_noise, noise)\n",
        "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=self.T_max)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def on_train_start(self):\n",
        "        # Clear the log file\n",
        "        open(self.log_file, 'w').close()\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg = self.trainer.callback_metrics['train_loss_epoch'].item()\n",
        "        self.train_losses.append(avg)\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(f\" Epoch {self.current_epoch + 1}: train_loss = {avg:.6f}\\n\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        avg = metrics.get('val_loss_epoch', metrics.get('val_loss', torch.tensor(0.0))).item()\n",
        "        self.val_losses.append(avg)\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(f\" Epoch {self.current_epoch + 1}: val_loss = {avg:.6f}\\n\")\n",
        "\n",
        "    def on_train_end(self):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(range(1, len(self.train_losses)+1), self.train_losses, label='Train', marker='o')\n",
        "        plt.plot(range(1, len(self.val_losses)+1), self.val_losses, label='Val', marker='s')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss Convergence')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.log_dir, 'loss_curve.png'))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yykl7Vgwuj9Y"
      },
      "outputs": [],
      "source": [
        "# del ddpm_model\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F45CGeHecem3"
      },
      "source": [
        "# Initialize the model and trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvPYOUsk5yaM"
      },
      "outputs": [],
      "source": [
        "ddpm_model = DDPM(num_steps=1000, lr=1e-4, T_max=100)\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val_loss',   # Monitor validation loss\n",
        "    patience=10,           # Stop after 5 epochs without improvement\n",
        "    verbose=True,\n",
        "    mode='min'            # Minimize the validation loss\n",
        ")\n",
        "\n",
        "# Define ModelCheckpoint callback to save model checkpoints during training\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='/content/drive/MyDrive/678_Team4_Outputs_new',  # Folder in Google Drive\n",
        "    filename='ddpm-{epoch:02d}-{val_loss:.5f}',\n",
        "    save_top_k=5,\n",
        "    mode='min',\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "# Learning Rate Monitor callback (to log learning rate at each step)\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50,\n",
        "    precision=\"16-mixed\",\n",
        "    devices=1 if torch.cuda.is_available() else 0,\n",
        "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(ddpm_model, train_loader, val_loader)\n",
        "trainer.validate(ddpm_model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suwCVfOy6DmL"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader for the test dataset\n",
        "test_dataset = TumorDataset(image_tensor=X_test, label_tensor=y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc8AHHggI6VJ"
      },
      "source": [
        "# Model Evaluation Using PyTorch Lightning Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lwoz7eDIZRh"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/678_Team4_Outputs_new/ddpm-epoch=29-val_loss=0.05851.ckpt'\n",
        "\n",
        "# Load the saved checkpoint\n",
        "ddpm_model = DDPM.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Initialize trainer for evaluation\n",
        "trainer = Trainer()\n",
        "\n",
        "# Evaluate on validation data\n",
        "trainer.validate(ddpm_model, dataloaders=val_loader)\n",
        "\n",
        "# Evaluate on test data (optional, after validation)\n",
        "trainer.test(ddpm_model, dataloaders=test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDdJATTI7Rvf"
      },
      "source": [
        "# Visualization and Saving of Original, Noisy, and Denoised Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxOG9PfwIqON"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create a directory to store the images if it doesn't exist\n",
        "output_folder = '/content/drive/MyDrive/678_Team4_Images/Images_final/'\n",
        "\n",
        "# Make sure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Get a batch of test images and labels\n",
        "images, labels = next(iter(test_loader))\n",
        "\n",
        "# Get noisy images using the forward process (noising)\n",
        "noisy_images, _ = ddpm_model.forward_process(images)\n",
        "\n",
        "# Get the model's predictions (denoised images)\n",
        "predictions = ddpm_model(images)\n",
        "\n",
        "# Normalize the images to the range [0, 1] for proper visualization\n",
        "def normalize_image(image):\n",
        "    # Clip the values between 0 and 1 for safe visualization\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return image\n",
        "\n",
        "batch_size = images.shape[0]\n",
        "\n",
        "# Save the original images, noisy images, and denoised predictions\n",
        "for i in range(batch_size):  # Show and save 30 images\n",
        "    # Plot original images\n",
        "    img = images[i].cpu().numpy().transpose(1, 2, 0)  # Convert from CHW to HWC\n",
        "    img = normalize_image(img)  # Normalize image for proper display\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True Label: {labels[i]}\")\n",
        "    plt.axis('off')\n",
        "    # Save the image\n",
        "    plt.savefig(f\"{output_folder}original_{i+1}_label_{labels[i]}.png\")\n",
        "    plt.clf()  # Clear the figure for the next plot\n",
        "\n",
        "    # Plot noisy images\n",
        "    noisy_img = noisy_images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "    noisy_img = normalize_image(noisy_img)  # Normalize noisy image\n",
        "    plt.imshow(noisy_img)\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis('off')\n",
        "    # Save the noisy image\n",
        "    plt.savefig(f\"{output_folder}noisy_{i+1}.png\")\n",
        "    plt.clf()  # Clear the figure for the next plot\n",
        "\n",
        "    # Plot denoised (predicted) images\n",
        "    pred_img = predictions[i].cpu().detach().numpy().transpose(1, 2, 0)\n",
        "    pred_img = normalize_image(pred_img)  # Normalize predicted image\n",
        "    plt.imshow(pred_img)\n",
        "    plt.title(f\"Pred: {labels[i]}\")\n",
        "    plt.axis('off')\n",
        "    # Save the predicted image\n",
        "    plt.savefig(f\"{output_folder}predicted_{i+1}_label_{labels[i]}.png\")\n",
        "    plt.clf()  # Clear the figure for the next plot\n",
        "\n",
        "print(f\"Images saved to: {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe0OKTUu8kLK"
      },
      "source": [
        "# Enhanced Image Saving Function for Model Output Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlOpG4ifdd6W"
      },
      "outputs": [],
      "source": [
        "# Enhanced saving function\n",
        "def save_samples(model, dataloader, device, output_folder, max_samples=100):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    saved_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "            if saved_count >= max_samples:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Process batch\n",
        "            t = torch.zeros(images.size(0), dtype=torch.long, device=device)\n",
        "            noisy_images, _, _ = model.forward_process(images, labels)\n",
        "            predictions = model(images)\n",
        "\n",
        "            # Save each image in batch\n",
        "            for i in range(images.size(0)):\n",
        "                if saved_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "                plt.figure(figsize=(12, 4))\n",
        "\n",
        "                # Original\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(images[i].cpu().permute(1, 2, 0).clip(0, 1))\n",
        "                plt.title(f\"Original (Label: {labels[i].item()})\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Noisy\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(noisy_images[i].cpu().permute(1, 2, 0).clip(0, 1))\n",
        "                plt.title(\"Noisy\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Denoised\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(predictions[i].cpu().permute(1, 2, 0).clip(0, 1))\n",
        "                plt.title(\"Denoised\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{output_folder}sample_{saved_count+1}.png\", bbox_inches='tight', dpi=100)\n",
        "                plt.close()\n",
        "\n",
        "                saved_count += 1\n",
        "\n",
        "    print(f\"Saved {saved_count} samples to {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4YlCpV8soJ"
      },
      "source": [
        "#Saving in a folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tagiO3Fddd3v"
      },
      "outputs": [],
      "source": [
        "# Usage - save first 100 samples\n",
        "save_samples(ddpm_model, test_loader, device, \"/content/drive/MyDrive/678_Team4_Images/Images_final2/\", max_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3aNQS6U83Vo"
      },
      "source": [
        "# Denoising Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tObupR8yXB35"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_denoising(model, dataloader, device, max_batches=None):\n",
        "    \"\"\"\n",
        "    Denoising evaluation (MSE + PSNR only)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'MSE': 0.0, 'PSNR': 0.0}\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
        "            if max_batches is not None and batch_idx >= max_batches:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            B = images.size(0)\n",
        "\n",
        "            # Generate noise + timestep\n",
        "            t = torch.randint(0, model.num_steps, (B,), device=device)\n",
        "            noise = torch.randn_like(images)\n",
        "\n",
        "            # Forward (diffusion) process\n",
        "            sqrt_alpha = model.alpha_cumprod[t].sqrt().view(-1, 1, 1, 1)\n",
        "            sqrt_1m_alpha = (1 - model.alpha_cumprod[t]).sqrt().view(-1, 1, 1, 1)\n",
        "            noisy = sqrt_alpha * images + sqrt_1m_alpha * noise\n",
        "\n",
        "            # Reverse (denoising) process\n",
        "            pred_noise = model.reverse_process(noisy, t, labels)\n",
        "            denoised = (noisy - pred_noise * sqrt_1m_alpha) / (sqrt_alpha + 1e-8)\n",
        "            denoised = denoised.clamp(0, 1)\n",
        "\n",
        "            # MSE + PSNR on GPU\n",
        "            mse = torch.mean((images - denoised) ** 2, dim=(1, 2, 3))\n",
        "            psnr = 10 * torch.log10(1.0 / (mse + 1e-10))\n",
        "\n",
        "            metrics['MSE'] += mse.sum().item()\n",
        "            metrics['PSNR'] += psnr.sum().item()\n",
        "            count += B\n",
        "\n",
        "    return {k: v / count for k, v in metrics.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qm5PAK_XB1C",
        "outputId": "20b5b896-7b47-462d-8bbd-7077ee4414cc"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate_denoising(ddpm_model, test_loader, device, max_batches=20)\n",
        "print(f\"Denoising Metrics: {metrics}\")\n",
        "print(f\"Denoising Metrics:\\nMSE: {metrics['MSE']:.4f}, PSNR: {metrics['PSNR']:.2f} dB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3fyBT1bKB9c"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    f\"Denoising Metrics: {metrics}\\n\"\n",
        "    f\"MSE: {metrics['MSE']:.4f}\\n\"\n",
        "    f\"PSNR: {metrics['PSNR']:.2f} dB\\n\"\n",
        ")\n",
        "\n",
        "# Save to file\n",
        "output_path = \"/content/drive/MyDrive/678_Team4_Images/denoising_metrics.txt\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(f\"✅ Metrics saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjX0E2fdPgk"
      },
      "source": [
        "#Evaluation of Classification Accuracy Using DDPM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l8g8R7TXByc"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification(model, dataloader, device, num_samples=2):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            preds = []\n",
        "            t = torch.randint(0, model.num_steps, (images.size(0),), device=device)\n",
        "            sqrt_alpha = model.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "            sqrt_one_minus_alpha = model.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "\n",
        "            for _ in range(num_samples):\n",
        "                noise = torch.randn_like(images)\n",
        "                noisy_images = sqrt_alpha * images + sqrt_one_minus_alpha * noise\n",
        "                pred_noise = model.reverse_process(noisy_images, t, labels)\n",
        "                preds.append(pred_noise)\n",
        "\n",
        "            avg_pred = torch.mean(torch.stack(preds), dim=0)\n",
        "            predicted_labels = (avg_pred.mean(dim=[1, 2, 3]) > 0.5).long()\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDec8G0IXBvk"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate_classification(ddpm_model, test_loader, device)\n",
        "print(f\"Classification Accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDywh5mPKodw"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    f\"Classification Accuracy: {accuracy}\\n\"\n",
        ")\n",
        "\n",
        "# Save to file\n",
        "output_path = \"/content/drive/MyDrive/678_Team4_Images/classification_accuracy.txt\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(f\"✅ Metrics saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSCpNZ8FdVlN"
      },
      "source": [
        "# Evaluation of Denoised Images Using PSNR and SSIM Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDUGhBbHXxoP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure your model is in eval mode and moved to the correct device\n",
        "ddpm_model.eval()\n",
        "ddpm_model.to(device)\n",
        "\n",
        "# Lists to store PSNR and SSIM scores\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "\n",
        "# Turn off gradients for evaluation\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, _ = batch  # Ignore labels if they are not used\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Denoised output from DDPM model\n",
        "        predictions = ddpm_model(images)  # Expected shape: [B, 3, H, W]\n",
        "\n",
        "        # Process each image in the batch individually\n",
        "        for i in range(images.size(0)):\n",
        "            # Ground truth image\n",
        "            gt = images[i].cpu().permute(1, 2, 0).numpy()  # CHW → HWC\n",
        "            # Denoised image\n",
        "            pred = predictions[i].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Normalize to [0, 1] for comparison\n",
        "            gt = np.clip(gt, 0, 1)\n",
        "            pred = np.clip(pred, 0, 1)\n",
        "\n",
        "            # PSNR\n",
        "            psnr = peak_signal_noise_ratio(gt, pred, data_range=1.0)\n",
        "            psnr_scores.append(psnr)\n",
        "\n",
        "            # SSIM\n",
        "            ssim = structural_similarity(gt, pred, channel_axis=-1, data_range=1.0)\n",
        "            ssim_scores.append(ssim)\n",
        "\n",
        "# Calculate and print average scores\n",
        "avg_psnr = np.mean(psnr_scores)\n",
        "avg_ssim = np.mean(ssim_scores)\n",
        "\n",
        "print(f\"\\n✅ Average PSNR: {avg_psnr:.2f}\")\n",
        "print(f\"✅ Average SSIM: {avg_ssim:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
